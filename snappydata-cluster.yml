# Launch Snappydata 1.0.0 cluster on k8s 1.9
# Submit using kubectl will create services for Locator, server and leader. 
# The services provide access to port 1527 for the locator and server. 
# The leader service opens the Pulse UI on port 5050
#
# Then, we launch 3 statefulSet pods each for Locator, server, Leader. 
# Scale using the replica count below. 
# Adjust CPU, memory to suit your needs. 
# 
apiVersion: v1
kind: Service
metadata:
  name: snappydata-locator
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  type: LoadBalancer
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  name: snappydata-locator-internal
  labels:
    app: snappydata
spec:
  ports:
  - port: 10334
    targetPort: 10334
    name: locator-clustering-endpoint
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  name: snappydata-server
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  type: LoadBalancer
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  name: snappydata-leader
  labels:
    app: snappydata
spec:
  ports:
  - port: 5050
    targetPort: 5050
    name: spark
  type: LoadBalancer
  selector:
    app: snappydata-leader

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: snappydata-locator
spec:
  serviceName: snappydata-locator
  replicas: 1
  selector:
    matchLabels:
      app: snappydata-locator
  template:
    metadata:
      labels:
        app: snappydata-locator
    spec:
      containers:
      - name: snappydata-locator
        image: snappydatainc/snappydata:1.0.0
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "1024Mi"
#           cpu: "200m"
        ports:
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
     # primitive liveness probe .. only useful when the JVM crashes
        livenessProbe:
          tcpSocket:
            port: 1527
          initialDelaySeconds: 80
        command: 
          - "/bin/bash"
          - "-c"
          - >
            cat /etc/passwd > /tmp/passwd;
            echo "$(id -u):x:$(id -u):$(id -g):dynamic uid:/opt/snappydata:/bin/false" >> /tmp/passwd;

            export NSS_WRAPPER_PASSWD=/tmp/passwd;
            export NSS_WRAPPER_GROUP=/etc/group;
            export LD_PRELOAD=/usr/lib64/libnss_wrapper.so;

            echo "starting sshd service";
            service sshd start;

            hname=`hostname`;
            workdir="/opt/snappydata/work/$hname";
            mkdir -p "$workdir";
            echo "$hname  -dir=$workdir" > /opt/snappydata/conf/locators;
            /opt/snappydata/sbin/snappy-locators.sh start;
            tail -f /dev/null;
        lifecycle:
          preStop:
            exec:
              command: ["/opt/snappydata/sbin/snappy-locators.sh", "stop"]
        volumeMounts:
        - mountPath: "/opt/snappydata/work"
          name: snappy-disk-claim

  volumeClaimTemplates:
  - metadata:
      name: snappy-disk-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
      storageClassName: standard
# A PV claim using 'standard' storage class. Will work only in Google container engine. 
# In other clouds, create a PV and assign storageClassName ...


---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: snappydata-server
spec:
  serviceName: snappydata-server
  replicas: 2
  selector:
    matchLabels:
      app: snappydata-server
  template:
    metadata:
      labels:
        app: snappydata-server
    spec:
      containers:
      - name: snappydata-server
        image: snappydatainc/snappydata:1.0.0
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "2048Mi"
#           cpu: "1000m"
        # Even servers use the same port as locator ... all run on independent pods
        # ... and, the service will either roundrobin or loadbalance
        ports:
        - containerPort: 1527
          name: jdbc
        livenessProbe:
          tcpSocket:
            port: 1527
#         initial delay intentionally kept large, as server waits(250 seconds) for locator to be available
          initialDelaySeconds: 360
        command:
          - "/bin/bash"
          - "-c"
          - >
            yum -y install nc && yum clean all -y;

            COUNTER=0;
            while ! nc -z snappydata-locator-internal 10334 > /dev/null 2>&1; do
              echo "Waiting for locator to be online";
              sleep 5;
              let COUNTER=COUNTER+1 ;
              if [ $COUNTER -eq 50 ]; then echo "ERROR: Locator is not up"; exit 1; fi;
            done;

            cat /etc/passwd > /tmp/passwd;
            echo "$(id -u):x:$(id -u):$(id -g):dynamic uid:/opt/snappydata:/bin/false" >> /tmp/passwd;

            export NSS_WRAPPER_PASSWD=/tmp/passwd;
            export NSS_WRAPPER_GROUP=/etc/group;
            export LD_PRELOAD=/usr/lib64/libnss_wrapper.so;

            echo "starting sshd service";
            service sshd start;

            hname=`hostname`;
            workdir="/opt/snappydata/work/$hname";
            mkdir -p "$workdir";
            echo "$hname -locators=snappydata-locator-internal:10334 -client-port=1527 -dir=$workdir" > /opt/snappydata/conf/servers;
            /opt/snappydata/sbin/snappy-servers.sh start;
            tail -f /dev/null;
        lifecycle:
          preStop:
            exec:
              command: ["/opt/snappydata/sbin/snappy-servers.sh", "stop"]
        volumeMounts:
        - mountPath: "/opt/snappydata/work"
          name: snappy-disk-claim

  volumeClaimTemplates:
  - metadata:
      name: snappy-disk-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
      storageClassName: standard
# A PV claim using 'standard' storage class. Will work only in Google container engine. 
# In other clouds, create a PV and assign storageClassName ...


---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: snappydata-leader
spec:
  serviceName: snappydata-leader
  replicas: 1
  selector:
    matchLabels:
      app: snappydata-leader
  template:
    metadata:
      labels:
        app: snappydata-leader
    spec:
      containers:
      - name: snappydata-leader
        image: snappydatainc/snappydata:1.0.0
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "1024Mi"
#            cpu: "1000m"
        ports:
        - containerPort: 5050
          name: sparkui
        livenessProbe:
          httpGet:
            path: /
            port: 5050
#         initial delay intentionally kept large, as lead waits(250 seconds) for servers to be available
          initialDelaySeconds: 360
        command: 
          - "/bin/bash"
          - "-c"
          - >
            yum -y install nc && yum clean all -y;

            COUNTER=0;
            while ! nc -z snappydata-server 1527 > /dev/null 2>&1; do
              echo "Waiting for servers to be online";
              sleep 5;
              let COUNTER=COUNTER+1 ;
              if [ $COUNTER -eq 50 ]; then echo "ERROR: Servers are not up"; exit 1; fi;
            done;

            cat /etc/passwd > /tmp/passwd;
            echo "$(id -u):x:$(id -u):$(id -g):dynamic uid:/opt/snappydata:/bin/false" >> /tmp/passwd;

            export NSS_WRAPPER_PASSWD=/tmp/passwd;
            export NSS_WRAPPER_GROUP=/etc/group;
            export LD_PRELOAD=/usr/lib64/libnss_wrapper.so;

            echo "starting sshd service";
            service sshd start;

            hname=`hostname`;
            workdir="/opt/snappydata/work/$hname";
            mkdir -p "$workdir";
            echo "$hname -locators=snappydata-locator-internal:10334 -dir=$workdir" > /opt/snappydata/conf/leads;
            /opt/snappydata/sbin/snappy-leads.sh start;
            tail -f /dev/null;
        lifecycle:
          preStop:
            exec:
              command: ["/opt/snappydata/sbin/snappy-leads.sh", "stop"]
        volumeMounts:
        - mountPath: "/opt/snappydata/work"
          name: snappy-disk-claim

  volumeClaimTemplates:
  - metadata:
      name: snappy-disk-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
      storageClassName: standard
# A PV claim using 'standard' storage class. Will work only in Google container engine. 
# In other clouds, create a PV and assign storageClassName ...

---
